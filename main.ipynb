{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install libraries (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n!pip install beir\\n!pip install fasttext\\n!pip install spacy\\n!pip scikit-learn\\n!pip install rank_bm25\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "!pip install beir\n",
    "!pip install fasttext\n",
    "!pip install spacy\n",
    "!pip scikit-learn\n",
    "!pip install rank_bm25\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maxim\\anaconda3\\envs\\research_ir\\Lib\\site-packages\\beir\\util.py:2: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import beir \n",
    "from beir import util, LoggingHandler\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "from beir.retrieval.evaluation import EvaluateRetrieval\n",
    "\n",
    "from utils_func import corpus_processing, matrix_creation, clustering, retriever_model, vector_creation\n",
    "\n",
    "import os\n",
    "import multiprocessing\n",
    "multiprocessing.set_start_method(\"spawn\", force=True)\n",
    "\n",
    "try:\n",
    "    import fasttext\n",
    "    import fasttext.util\n",
    "except:\n",
    "    print('fasttext not imported')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"nfcorpus\"                        # dataset you want to use, had to be available in the beir benchmark: https://github.com/beir-cellar/beir\n",
    "\n",
    "use_ft = True                               # whether to use fasttext or not to handle unseen words\n",
    "path_ft = 'cc.en.100.bin'                   # path to the fasttext model, if empty and use_ft is true, the model will be downloaded in the current directory\n",
    "save_cleaned_corpus = ''                    # path to save the cleaned corpusn, if empty, the corpus will not be saved\n",
    "save_scores = ''                            # path to save the scores, if empty, the scores will not be saved \n",
    "\n",
    "load_cleaned_corpus = ''                    # path to load the cleaned corpus, if empty, the corpus will be cleaned\n",
    "load_vectors = f'word_vectors/word_vectors_{dataset}.csv' # path to load the word vectors, if empty, the vectors will be created\n",
    "vector_dimension = 100                      # dimension of the word vectors\n",
    "path_to_save_model = ''                     # path to save the fasttext model trained on the corpora, if empty, the model will not be saved\n",
    "\n",
    "remove_original_corpus = True               # whether to remove the original corpus from the memory or not\n",
    "\n",
    "best_n_neighbors = 75                       # number of neighbors to consider to fill the similarity matrix\n",
    "best_alpha = 0.76                           # alpha parameter, balancing the importance between similarity and coexistence\n",
    "best_thresh = 0.75                          # threshold to consider a word as replaceable by another one\n",
    "metric = 'cosine'                           # metric to use to compute the similarity matrix   \n",
    "k1 = 1.5                                    # parameter of the BM25 algorithm \n",
    "b = 0.75                                    # parameter of the BM25 algorithm\n",
    "thresh_prob=0.005                           # threshold to consider a value equals to 0 in the coexistence matrix           \n",
    "\n",
    "knn_method = 'faiss'                        # method to use to compute the k-ne, either 'faiss' or 'exact' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3633/3633 [00:00<00:00, 152607.98it/s]\n"
     ]
    }
   ],
   "source": [
    "url = \"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{}.zip\".format(dataset)\n",
    "if not os.path.exists(f\"datasets/\"):\n",
    "    os.makedirs(f\"datasets/\")\n",
    "    \n",
    "data_path = f\"datasets/{dataset}\"\n",
    "corpus, queries, qrels = GenericDataLoader(data_folder=data_path).load(split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    if use_ft:\n",
    "        fasttext_model = fasttext.load_model(path_ft)\n",
    "    else:\n",
    "        fasttext_model = None\n",
    "except:\n",
    "    print('Model not found')\n",
    "    if use_ft:\n",
    "        print('Downloading model...')\n",
    "        fasttext.util.download_model('en', if_exists='ignore')  # English\n",
    "        fasttext_model = fasttext.load_model('cc.en.300.bin')\n",
    "        print('Reducing model...')\n",
    "        if vector_dimension != 300:\n",
    "            fasttext.util.reduce_model(fasttext_model, vector_dimension)\n",
    "        print('Saving model...')\n",
    "        if path_ft != '':\n",
    "            fasttext_model.save_model(path_ft)\n",
    "        fasttext_model.save_model(f'cc.en.{vector_dimension}.bin')\n",
    "        print('Model saved.')\n",
    "    \n",
    "    else:\n",
    "        fasttext_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prétraitement du corpus: 100%|██████████| 3633/3633 [00:28<00:00, 125.54it/s]\n"
     ]
    }
   ],
   "source": [
    "if load_cleaned_corpus == '':\n",
    "    cleaned_corpus = corpus_processing.preprocess_corpus_dict(corpus)\n",
    "    if save_cleaned_corpus != '':\n",
    "        corpus_processing.save_processed_corpus(cleaned_corpus, save_cleaned_corpus)\n",
    "else:\n",
    "    cleaned_corpus = pd.read_csv(load_cleaned_corpus)\n",
    "    cleaned_corpus = {cleaned_corpus['doc_id'][i]:cleaned_corpus['text'][i] for i in range(len(cleaned_corpus))}\n",
    "\n",
    "if remove_original_corpus:\n",
    "    corpus = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating file to train fasttext model:   0%|          | 0/3633 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating file to train fasttext model: 100%|██████████| 3633/3633 [00:00<00:00, 171607.71it/s]\n",
      "Getting unique words: 100%|██████████| 3633/3633 [00:00<00:00, 45191.67it/s]\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(load_vectors):\n",
    "    embeddings = pd.read_csv(load_vectors, sep=' ',na_values=[''], keep_default_na=False, index_col=0).dropna()\n",
    "    for i in list(embeddings.columns)[1:]:\n",
    "        embeddings[i] = embeddings[i].astype(float)\n",
    "else:\n",
    "    embeddings = vector_creation.create_vectors(cleaned_corpus, vector_dimension, path_to_save_vectors=load_vectors, path_to_save_model=path_to_save_model, fasttext_model=fasttext_model, epochs = 5, model = 'skipgram')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting unique words: 100%|██████████| 3633/3633 [00:00<00:00, 49678.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing embeddings for cosine similarity...\n",
      "Embeddings normalized.\n",
      "Creating Faiss index...\n",
      "Faiss index created.\n",
      "getting distances\n",
      "end of getting distances\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "filling similarity matrix: 100%|██████████| 28549/28549 [00:03<00:00, 8426.76it/s]\n",
      "Getting unique words: 100%|██████████| 3633/3633 [00:00<00:00, 59999.55it/s]\n",
      "Getting word presence: 100%|██████████| 3633/3633 [00:00<00:00, 27311.89it/s]\n",
      "Calculating coexistence probability: 100%|██████████| 28549/28549 [00:26<00:00, 1070.19it/s]\n",
      "Getting replaceable words: 100%|██████████| 28549/28549 [00:08<00:00, 3343.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding cycles...\n",
      "end of finding cycles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating clusters dict: 100%|██████████| 21543/21543 [00:03<00:00, 7135.27it/s]\n",
      "Rewriting corpus: 100%|██████████| 3633/3633 [00:06<00:00, 531.71it/s]\n",
      "tests in progress: 100%|██████████| 323/323 [00:14<00:00, 22.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'NDCG@1': 0.43034, 'NDCG@3': 0.39118, 'NDCG@5': 0.36141, 'NDCG@10': 0.33487, 'NDCG@100': 0.28527, 'NDCG@1000': 0.34877}, {'MAP@1': 0.05798, 'MAP@3': 0.09424, 'MAP@5': 0.10771, 'MAP@10': 0.12535, 'MAP@100': 0.15023, 'MAP@1000': 0.16125}, {'Recall@1': 0.05798, 'Recall@3': 0.10209, 'Recall@5': 0.12967, 'Recall@10': 0.1636, 'Recall@100': 0.26176, 'Recall@1000': 0.47483}, {'P@1': 0.43963, 'P@3': 0.36326, 'P@5': 0.30341, 'P@10': 0.24427, 'P@100': 0.06882, 'P@1000': 0.01782})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scores = {}\n",
    "retriever = retriever_model.UCFIRe(embeddings, fasttext_model,n_neighbors = best_n_neighbors, alpha=best_alpha, thresh = best_thresh, metric = metric, k1 = k1, b = b, thresh_prob=thresh_prob)\n",
    "\n",
    "retriever.fit(cleaned_corpus, is_clean=True, knn_method=knn_method)\n",
    "retriever_okapi = EvaluateRetrieval(retriever, score_function=\"cos_sim\") # or \"dot\" if you wish dot-product\n",
    "results_okapi = retriever_okapi.retrieve(retriever.tokenized_corpus, queries)\n",
    "# Evaluate the model (implement your own evaluation logic, e.g., compute mean reciprocal rank)\n",
    "scores = retriever_okapi.evaluate(qrels, results_okapi, retriever_okapi.k_values)  # Replace this with your evaluation metric\n",
    "\n",
    "if save_scores != '':\n",
    "    with open(save_scores, 'w') as f:\n",
    "        f.write(str(scores))\n",
    "    \n",
    "print(scores)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_ir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
